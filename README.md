# ğŸ§¿ Asistente Multimodal para Personas con Discapacidad Visual

> Proyecto de fin de mÃ¡ster en Inteligencia Artificial, Data Science y Big Data
> AplicaciÃ³n accesible e inteligente capaz de asistir a personas con discapacidad visual mediante entrada por voz e imagen.

---

## ğŸŒ Objetivo del proyecto

El objetivo principal es **desarrollar un asistente multimodal accesible** que pueda ayudar a personas con discapacidad visual a **comprender y realizar tareas del entorno fÃ­sico**, como:

- ğŸ“¸ Describir escenas visibles desde una imagen.
- ğŸ” Localizar objetos especÃ­ficos.
- ğŸ“ƒ Leer textos en documentos o pantallas (OCR).
- ğŸ—£ï¸ Conversar por voz y obtener respuestas tambiÃ©n por voz.

Este asistente toma **una imagen (o vÃ­deo) y un mensaje de audio** del usuario, detecta la intenciÃ³n y activa el modelo especializado que mejor puede resolverla.

---

## ğŸ§  Â¿CÃ³mo funciona?

El sistema estÃ¡ basado en una arquitectura de agente **LangGraph**, donde:

- Un **LLM orquestador** decide **quÃ© modelo usar** segÃºn la consulta del usuario.
- Cada **nodo especializado** resuelve una tarea concreta de visiÃ³n o comprensiÃ³n.
- Se gestionan las entradas/salidas mediante una interfaz web accesible con Gradio.


[Entrada Usuario] --> (Audio + Imagen)
|
v
[TranscripciÃ³n del audio (STT)]
|
v
[LLM Orquestador (LangGraph)]
|
v
[Nodo especializado en cierta tarea]
|
v
[Respuesta en texto] 
|
v
[ConversiÃ³n a audio (TTS)] 
|
v
[Salida al usuario]

## ğŸ“ Estructura del Repositorio
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ main.py # InicializaciÃ³n del agente e interfaz Gradio
â”‚ â”œâ”€â”€ router.py # Endpoints API opcionales
â”‚ â”œâ”€â”€ config.py # ConfiguraciÃ³n general del sistema
â”‚ â””â”€â”€ utils.py # Funciones auxiliares
â”‚
â”œâ”€â”€ agent/
â”‚ â”œâ”€â”€ langgraph_orchestrator.py # Orquestador con LangGraph
â”‚ â”œâ”€â”€ prompt_templates.py # Prompts del LLM orquestador
â”‚ â””â”€â”€ memory_manager.py # GestiÃ³n opcional de memoria conversacional
â”‚
â”œâ”€â”€ nodes/
â”‚ â”œâ”€â”€ scene_description.py # Nodo de descripciÃ³n de escenas
â”‚ â”œâ”€â”€ ocr_extractor.py # Nodo de reconocimiento de texto
â”‚ â”œâ”€â”€ object_localizer.py # Nodo de detecciÃ³n de objetos
â”‚ â””â”€â”€ common_models.py # Carga compartida de modelos
â”‚
â”œâ”€â”€ preprocessing/
â”‚ â”œâ”€â”€ speech_to_text.py # ConversiÃ³n de audio a texto
â”‚ â”œâ”€â”€ image_utils.py # Preprocesado de imÃ¡genes
â”‚ â””â”€â”€ text_to_speech.py # ConversiÃ³n de texto a voz
â”‚
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ gradio_interface.py # Interfaz visual de usuario
â”‚
â”œâ”€â”€ models/ # (Opcional) Modelos fine-tuned especÃ­ficos
â”‚
â”œâ”€â”€ tests/
â”‚ â”œâ”€â”€ test_nodes.py # Unit tests de nodos
â”‚ â”œâ”€â”€ test_agent.py # Test del orquestador
â”‚ â””â”€â”€ test_integration.py # Tests end-to-end del pipeline
â”‚
â”œâ”€â”€ Dockerfile # Entorno de ejecuciÃ³n portÃ¡til
â””â”€â”€ README.md # DocumentaciÃ³n general

## ğŸš€ CÃ³mo ejecutar el proyecto
